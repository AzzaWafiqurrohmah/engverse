{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlzPQaLt3Wkt"
   },
   "source": [
    "# Section - Listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvsIW4MyPIx9",
    "outputId": "2e0048e0-410f-47cd-c76d-8a58e0cd5de5"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'base (Python 3.12.7)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOSPC: no space left on device, write"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Set seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load data\n",
    "with open('data/listening.json', 'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Simulasi user\n",
    "data = []\n",
    "n_users_per_label = 50\n",
    "\n",
    "for label in ['Short Conversation', 'Extended Conversation', 'Talk']:\n",
    "    for _ in range(n_users_per_label):\n",
    "        user_results = []\n",
    "        error_probs = {\n",
    "            'Short Conversation': np.random.uniform(0.1, 0.2),\n",
    "            'Extended Conversation': np.random.uniform(0.1, 0.2),\n",
    "            'Talk': np.random.uniform(0.1, 0.2),\n",
    "        }\n",
    "        error_probs[label] = np.random.uniform(0.6, 0.8)\n",
    "\n",
    "        for q in questions:\n",
    "            part = q['part']\n",
    "            correct = np.random.rand() > error_probs[part]\n",
    "            user_results.append((part, correct))\n",
    "\n",
    "        stats = {}\n",
    "        for part in ['Short Conversation', 'Extended Conversation', 'Talk']:\n",
    "            part_results = [r for r in user_results if r[0] == part]\n",
    "            total = len(part_results)\n",
    "            wrong = sum(not r[1] for r in part_results)\n",
    "            stats[f\"{part}_err\"] = wrong / total if total > 0 else 0\n",
    "\n",
    "        weak_part = max(stats, key=stats.get).replace('_err', '')\n",
    "        data.append({\n",
    "            'short_err': stats['Short Conversation_err'],\n",
    "            'extended_err': stats['Extended Conversation_err'],\n",
    "            'talk_err': stats['Talk_err'],\n",
    "            'weak_part': weak_part\n",
    "        })\n",
    "\n",
    "# DataFrame dan fitur\n",
    "df = pd.DataFrame(data)\n",
    "df['short_vs_ext'] = df['short_err'] - df['extended_err']\n",
    "df['short_vs_talk'] = df['short_err'] - df['talk_err']\n",
    "df['ext_vs_talk'] = df['extended_err'] - df['talk_err']\n",
    "\n",
    "X = df[['short_err', 'extended_err', 'talk_err', 'short_vs_ext', 'short_vs_talk', 'ext_vs_talk']].values\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(df['weak_part'])\n",
    "y = tf.keras.utils.to_categorical(y_int, num_classes=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y_int, random_state=SEED\n",
    ")\n",
    "\n",
    "# Arsitektur model sederhana dengan regularisasi\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(6,)),\n",
    "    tf.keras.layers.Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(12, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callback untuk EarlyStopping dan learning rate decay\n",
    "# Callback khusus: stop ketika val_accuracy >= 0.85\n",
    "class CustomEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = logs.get('val_accuracy')\n",
    "        if val_acc is not None and val_acc >= 0.85:\n",
    "            print(f\"\\n‚úÖ Validation accuracy {val_acc:.2f} >= 0.85. Stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1\n",
    "    ),\n",
    "    CustomEarlyStopping()  # Tambahkan di sini\n",
    "]\n",
    "\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluasi\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nüéØ Test Accuracy: {acc:.2f}\")\n",
    "\n",
    "# Prediksi\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "y_true = y_test.argmax(axis=1)\n",
    "\n",
    "# Report\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "99O1hKFW1mdy",
    "outputId": "1a675ec3-f234-42ea-eed0-f819e9740f44"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualisasi hasil evaluasi model\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot akurasi\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WErBIM67oqQ2",
    "outputId": "cdab6346-fe10-4264-fea7-0b4a53c27d54"
   },
   "outputs": [],
   "source": [
    "# 12. Simpan model dan label encoder\n",
    "model.save('model/listening_classifier.h5')\n",
    "import joblib\n",
    "joblib.dump(le, 'model/listening_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPkRAAgprdWL",
    "outputId": "935eae56-1b8e-4874-8d5b-7a325e84bd82"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# Load model dan label encoder\n",
    "model = tf.keras.models.load_model('model/listening_classifier.h5')\n",
    "le = joblib.load('model/listening_encoder.pkl')\n",
    "\n",
    "# Contoh error rate user\n",
    "short, ext, talk = 0.1, 0.1, 0.8\n",
    "features = np.array([[short, ext, talk,\n",
    "                      short - ext, short - talk, ext - talk]])\n",
    "\n",
    "# Prediksi\n",
    "pred = model.predict(features)\n",
    "pred_probs = pred[0]\n",
    "label_index = np.argmax(pred_probs)\n",
    "pred_label = le.inverse_transform([label_index])[0]\n",
    "\n",
    "# Output\n",
    "print(f\"\\n‚úÖ Prediksi kelemahan user: {pred_label}\")\n",
    "print(f\"üìä Probabilitas prediksi: {pred_probs}\")\n",
    "print(f\"üè∑Ô∏è Label indeks: {label_index}, Label kelas: {le.classes_[label_index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iyWqB-WtagH"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
